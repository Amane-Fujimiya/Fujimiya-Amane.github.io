After frenzy hiccup and finally a serious quarrel with codes and C++, I have came back. Definitely. 

But for now, I am a little bit confused of what to do in the future, to the point that I actually think it is better to not do anything at all, and maybe perhaps working on self-improvement instead of chasing my own tales of success. 
Laying off seems to be better I guess.

# News Update[^1]
I will start doing actual research catalogue, maybe later this date. 
# Research Recommendation[^2]
___
## QOTD (Question of the day)

- Mathematics is like the language, more formal. Then what and which language of mathematics would it be at best perfect of all assumption and framework that it depends upon? When will the fallacy within such language if ever, will result in the invalidation of such formal language? 
- Under how many circumstances that the general notion $P(A\cap B)=P(B\cap A)$ fails?
- A particular approach to mathematics is by setting a certain number of axioms, and their logic, then the general "environment" such mathematics is placed on, of which then formed category (or mathematical structures), and then proceed from that on? Can we process mathematics rigorously as such, couple with the structured approach and notion of "abstraction" (borrowed from computer programming) for later layer of mathematics? 
- What is CSR Representation of matrix information, and how can I couple that with the data type tensors of higher order? 
- How can we interpret the ENCODE database of human genes? And how can we simulate such gene back into the generative patterns that create human?
## TOTD (Thoughts of the day)
___
I have a novelty opinion on artificial intelligence framework of mine, for real. Given an environment $\mathcal{O}$, of which contains several objects $\{ O_{1},\dots O_{m}\}$ which hold details macroscopic/microscopic details about such environment, we define a hidden generative patterns/ruleset $R(\mathcal{O})$ of which the world is generated. We then can evaluate any specific agent $\mathcal{A}[P]$ with a default model $P$ into such environment, accompanied by the evolutional process for such agent. 

This is unfinished, but it seems quite good. The problem is to gauge my how much is the efficiency of such agent, and their generational configuration. Furthermore, how many model $P$ are there for testing, and how many rules would $R(\mathcal{O})$ be able to simulate is quite concerning. Furthermore, again, if we are taking in concepts of PAC learning model, we need to have such evaluation metric for learning closed bound, and $poly(\delta,\epsilon,m,n)$ of which gauges the agent within polynomial time of operation. So definitely a lot of problem is happening, and this system feel a bit overly-complicated. 

But that is not that much from the question of how and why would increase the neural node number from $n$ to $n+1$ will increase their efficiency, apparently, I guess.
### References
Learning Resources: [Stanford](https://web.stanford.edu/class/stats200/) - Introduction to Statistical Interference.
